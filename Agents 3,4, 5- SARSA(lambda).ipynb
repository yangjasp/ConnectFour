{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f0b4d2",
   "metadata": {},
   "source": [
    "# Agent 3 - Tabular Sarsa(位)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97d26b",
   "metadata": {},
   "source": [
    "Agent 2 (MC Method) worked reasonably well. After around playing around 50,000 training games, it learned a policy that could be used to consistently beat the random agent and also beat the `negamax` agent over 50% of the time. Still, this agent's abilities are limited by the fact that it only updates the value of each state after entire episodes. We have learned that this approach is not always optimal, and so n-step TD methods (which update the reward for a state based on the next n-steps) are often better. \n",
    "\n",
    "n-step TD have two notable limitations themselves: they take alot of memory and have a delay in updating. TD(位) methods, which give the overall return for a step as a weighted combination of various n-step TD returns, avoid these issues. In this section, I implement SARSA(位), a control version of TD(位) that learns action values instead of state values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e33c6",
   "metadata": {},
   "source": [
    "## Step 1: Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b8e42de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle-environments in ./opt/anaconda3/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in ./opt/anaconda3/lib/python3.9/site-packages (from kaggle-environments) (4.4.0)\n",
      "Requirement already satisfied: Flask>=1.1.2 in ./opt/anaconda3/lib/python3.9/site-packages (from kaggle-environments) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./opt/anaconda3/lib/python3.9/site-packages (from kaggle-environments) (1.21.5)\n",
      "Requirement already satisfied: requests>=2.25.1 in ./opt/anaconda3/lib/python3.9/site-packages (from kaggle-environments) (2.27.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in ./opt/anaconda3/lib/python3.9/site-packages (from Flask>=1.1.2->kaggle-environments) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in ./opt/anaconda3/lib/python3.9/site-packages (from Flask>=1.1.2->kaggle-environments) (2.0.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in ./opt/anaconda3/lib/python3.9/site-packages (from Flask>=1.1.2->kaggle-environments) (2.0.1)\n",
      "Requirement already satisfied: click>=5.1 in ./opt/anaconda3/lib/python3.9/site-packages (from Flask>=1.1.2->kaggle-environments) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=2.10.1->Flask>=1.1.2->kaggle-environments) (2.0.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in ./opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0.1->kaggle-environments) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0.1->kaggle-environments) (21.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/lib/python3.9/site-packages (from requests>=2.25.1->kaggle-environments) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./opt/anaconda3/lib/python3.9/site-packages (from requests>=2.25.1->kaggle-environments) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.9/site-packages (from requests>=2.25.1->kaggle-environments) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./opt/anaconda3/lib/python3.9/site-packages (from requests>=2.25.1->kaggle-environments) (2.0.4)\n",
      "['random', 'negamax']\n",
      "['random', 'negamax']\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "##### Step 1: Load Environment\n",
    "\n",
    "!pip install kaggle-environments\n",
    "\n",
    "###### Import necessary libraries (run this twice per instructions)\n",
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "# Create the game environment\n",
    "# Set debug=True to see the errors if your agent refuses to run\n",
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "# List of available default agents\n",
    "print(list(env.agents))\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "\n",
    "# Create the game environment\n",
    "# Set debug=True to see the errors if your agent refuses to run\n",
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "# List of available default agents\n",
    "print(list(env.agents))\n",
    "\n",
    "##### Other libraries and functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm # to keep track of the progress\n",
    "from collections import defaultdict  # we will see why this is something useful for our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "220a3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "##### Define environment parameters, make environment\n",
    "#####\n",
    "rows = 4\n",
    "columns = 5\n",
    "in_a_row = 3\n",
    "debug_mode = True\n",
    "\n",
    "\n",
    "env = make(\"connectx\", {\"rows\": rows, \"columns\": columns, \"inarow\": in_a_row}, steps=[], debug=debug_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd19013",
   "metadata": {},
   "source": [
    "## Step 2: Bring in helper functions from code for Agent 2: MC Method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3391dc",
   "metadata": {},
   "source": [
    "For taking information about state from game, generating episodes, etc. For documentation on these functions see the Agent 2 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "9669d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_id(Struct):\n",
    "    return (tuple(Struct['board']), Struct.mark) # Combine the two features we need to know: the state of the board and which mark we are\n",
    "    # into a tuple\n",
    "    \n",
    "    # Note, Struct.board worked when the trainer was set as [None, other], but not when it was [other, None].\n",
    "    # No idea why, but I get around this by using Struct['board']\n",
    "    \n",
    "def epsilon_soft(env, Q, epsilon=0.25): # set 0.25 as default here\n",
    "    nA = env.configuration.columns\n",
    "    # policy = defaultdict(lambda: np.ones(nA)*epsilon/nA) # default policy for non-greedy action is random \n",
    "    policy = defaultdict(lambda: np.ones(nA)/nA)\n",
    "    for keys, values in Q.items():\n",
    "        # Now find best action *out of valid moves*\n",
    "        valid_moves = [col for col in range(nA) if state[0][col] == 0] # Find valid moves. Remember state[0] is the board\n",
    "        n_valid_moves = len(valid_moves) # number of valid moves\n",
    "            \n",
    "        for i in range(len(policy[keys])): # Need to define policy so that illegal moves can't be taken\n",
    "            if i in valid_moves:\n",
    "                policy[keys][i] = epsilon/n_valid_moves\n",
    "            else:\n",
    "                policy[keys][i] = 0\n",
    "        \n",
    "        \n",
    "        valid_Q_state = Q[keys][:]   # I think this is a messy way to do this but it should work. \n",
    "        for i in range(len(valid_Q_state)):\n",
    "            if i not in valid_moves:\n",
    "                valid_Q_state[i] = -1000000 # Set Q for illegal moves extremely low so it won't be max\n",
    "        best_action = np.argmax(valid_Q_state) # Find best action\n",
    "        policy[keys][best_action] = 1- epsilon + epsilon/n_valid_moves # and update probability for the best move\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### OLD BELOW. PROBLEM HERE WAS INVALID ACTION SOMETIMES HAD BEST Q, so I get around this by\n",
    "        ### assigning extremely low Q to the invalid actions so it will never be best\n",
    "        \n",
    "        #best = np.argmax(values) # find the best action\n",
    "        #valid_moves = [col for col in range(columns) if keys[0][col] == 0] # Find valid moves. key[0] is the board\n",
    "        #n_valid_moves = len(valid_moves) # number of valid moves\n",
    "    \n",
    "        #for i in range(len(policy[keys])): # Need to define policy so that illegal moves can't be taken\n",
    "        #    if i in valid_moves:\n",
    "        #        policy[keys][i] = epsilon/n_valid_moves\n",
    "        #    else:\n",
    "        #        policy[keys][i] = 0\n",
    "    \n",
    "        # policy[keys][best] = 1-epsilon + epsilon/n_valid_moves # and the greedy action will be take with prob 1 - 0.25 + epsilon/nA\n",
    "    return policy \n",
    "\n",
    "# ADD THIS FOR SARSA FUNCTION\n",
    "def epsilon_soft_state_specific(env, board, Q_state, epsilon=0.25): # set 0.25 as default here\n",
    "    nA = env.configuration.columns\n",
    "    new_policy = np.ones(nA)/nA\n",
    "    for values in Q_state:\n",
    "        # Find best action out of valid ones\n",
    "        valid_moves = [col for col in range(nA) if board[col] == 0] # Find valid moves\n",
    "        n_valid_moves = len(valid_moves) # number of valid moves\n",
    "    \n",
    "        for i in range(len(Q_state)): # Need to define policy so that illegal moves can't be taken\n",
    "            if i in valid_moves:\n",
    "                new_policy[i] = epsilon/n_valid_moves\n",
    "            else:\n",
    "                new_policy[i] = 0\n",
    "        \n",
    "        valid_Q_state = Q_state[:]   # I think this is a messy way to do this but it should work. \n",
    "        for i in range(len(valid_Q_state)):\n",
    "            if i not in valid_moves:\n",
    "                valid_Q_state[i] = -1000000 # Set Q for illegal moves extremely low so it won't be max\n",
    "        best_action = np.argmax(valid_Q_state) # Find best action\n",
    "    \n",
    "        new_policy[best_action] = 1-epsilon + epsilon/n_valid_moves # and the greedy action will be take with prob 1 - 0.25 + epsilon/nA\n",
    "    return new_policy \n",
    "\n",
    "# Note that this uses the trainer, not just the environment\n",
    "def generate_episode(trainer, Q, policy):\n",
    "    nA = env.configuration.columns # number of actions is the number of columns on the board\n",
    "    episode = [] # initialize the episode as an empty list. We will add to it later\n",
    "    state = state_id(trainer.reset()) # empty board\n",
    "    while True:\n",
    "        if state in Q:\n",
    "            action = np.random.choice(np.arange(nA), p = policy[state]) \n",
    "        else:\n",
    "            # if not already in dict, take random action. But make sure it is a legal action\n",
    "            valid_moves = [col for col in range(nA) if state[0][col] == 0]\n",
    "            n_valid_moves = len(valid_moves) # number of valid moves\n",
    "            action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "        action = int(action) # Make it an integer, not a numpy integer\n",
    "        next_state, reward, done, info = trainer.step(action)\n",
    "        next_state = state_id(next_state) # convert state to tuple for using as dictionary\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break # finish episode when environment says game is done. No need to set a turn limit,\n",
    "            # because max number of turns per game is 20.\n",
    "    return episode\n",
    "\n",
    "# Plot rewards function from Lab 8\n",
    "def plot_rewards(cum_rew, method=None):\n",
    "    plt.plot(cum_rew)\n",
    "    plt.xlabel(\"episode\")\n",
    "    plt.ylabel(r\"$\\sum R$\")\n",
    "    plt.title(method)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd642a",
   "metadata": {},
   "source": [
    "## Step 3: Define agent class and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c53e84",
   "metadata": {},
   "source": [
    "This agent will look very similar to the `MCMethodAgent`. Note that we now need to define an eligibility trace for each state action pair (initialized as 0 for each). Note that the learning algorithm follows roughly from Figure 7.11 in version 1 of the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "02081962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularSARSA(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        from collections import defaultdict\n",
    "        self.epsilon = 0.25\n",
    "        self.e = defaultdict(lambda: np.zeros(5)) # initialize e(s,a) = 0 for all s, a\n",
    "        self.Q = defaultdict(lambda: np.zeros(5)) # initialize Q as 0 for all s, a\n",
    "        self.env = make(\"connectx\", {\"rows\": 4, \"columns\": 5, \"inarow\": 3}, steps=[], debug=debug_mode)\n",
    "        self.policy = epsilon_soft(self.env, self.Q) # initial policy\n",
    "        self.greedy_policy = self.policy # this is firm policy for\n",
    "        self.cumulative_reward = [] # initialize list holding cumulative rewards from learning\n",
    "\n",
    "    def act(self, obs):  # play (greedily) according to the current policy. Note that it only plays one turn. \n",
    "        board_id = (tuple(obs.board), obs.mark,)\n",
    "        if board_id in self.policy.keys():\n",
    "            return int(np.argmax(self.policy[board_id]))   \n",
    "        else:\n",
    "            valid_moves = [col for col in range(5) if obs.board[col] == 0]\n",
    "            n_valid_moves = len(valid_moves) # number of valid moves\n",
    "            action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "            action = int(action) # Make it an integer, not a numpy integer\n",
    "            #print('playing random')\n",
    "            return action\n",
    "        \n",
    "    def act_nongreedily(self, obs):\n",
    "        board_id = (tuple(obs.board), obs.mark,)\n",
    "        if board_id in self.policy.keys():\n",
    "            action = np.random.choice(range(5), p = self.policy[board_id]) \n",
    "            return action     \n",
    "        else:\n",
    "            valid_moves = [col for col in range(5) if obs.board[col] == 0]\n",
    "            n_valid_moves = len(valid_moves) # number of valid moves\n",
    "            action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "            action = int(action) # Make it an integer, not a numpy integer\n",
    "            #print('playing random')\n",
    "            return action\n",
    "        \n",
    "            \n",
    "\n",
    "    def learn_TabularSARSA(self, trainer_mark1, trainer_mark2, num_episodes = 10000, \n",
    "                         gamma = 0.9, epsilon = 0.25, alpha = 0.1, lmbda = 0.5): \n",
    "                \n",
    "        # Initialize env, e, Q, policy\n",
    "        env = self.env\n",
    "        e = self.e.copy()\n",
    "        Q = self.Q.copy()\n",
    "        policy = self.policy.copy()\n",
    "        cumulative_reward = self.cumulative_reward.copy()\n",
    "        \n",
    "        nA = env.configuration.columns # number of actions is the number of columns on the board\n",
    "    \n",
    "        # loop over episodes\n",
    "        for i in tqdm(range(num_episodes)): # tgdm gives progress bar\n",
    "            episode = [] # initialize the episode as an empty list. We will add to it later\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                state = state_id(trainer_mark1.reset()) # empty board\n",
    "                current_trainer = trainer_mark1\n",
    "                mark = int(1)\n",
    "            else:\n",
    "                state = state_id(trainer_mark2.reset()) # empty board\n",
    "                current_trainer = trainer_mark2\n",
    "                mark = int(2)\n",
    "                \n",
    "  \n",
    "            # Initialize action\n",
    "            action = int(np.random.choice(np.arange(nA), p = policy[state]))  \n",
    "                                    # don't have to worry about illegal first action\n",
    "            \n",
    "            \n",
    "            # Repeat, for each step in the episode:\n",
    "            while True: \n",
    "                next_state_raw, reward, done, info = current_trainer.step(action)\n",
    "                next_state = state_id(next_state_raw)\n",
    "            \n",
    "                if done == True: # If step is terminal\n",
    "                    #next_state = state[:] # if this is a terminal state, there is no next_state\n",
    "                    #next_action = action.copy()\n",
    "                    delta = reward - gamma*Q[state][action]\n",
    "                    cumulative_reward.append(reward)\n",
    "                    \n",
    "                else:\n",
    "                    if next_state in Q:\n",
    "                        next_action = np.random.choice(np.arange(nA), p = policy[next_state]) \n",
    "                    else:\n",
    "                        # if not already in dict, take random action. But make sure it is a legal action\n",
    "                        valid_moves = [col for col in range(nA) if next_state[0][col] == 0]\n",
    "                        n_valid_moves = len(valid_moves) # number of valid moves\n",
    "                        next_action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "                    next_action = int(next_action) # Make it an integer, not a numpy integer\n",
    "                   \n",
    "                    delta = reward + gamma*Q[next_state][next_action] - Q[state][action] # update delta\n",
    "                \n",
    "                e[state][action] = e[state][action] + 1\n",
    "             \n",
    "                # and now update for all s, a (clearly if already 0, they stay \n",
    "                # 0 so only need to updating existing entries):\n",
    "                for mystate in Q.keys():\n",
    "                    for myaction in range(nA):\n",
    "                        Q[mystate][myaction] = Q[mystate][myaction] + alpha*delta*e[mystate][myaction]\n",
    "                        e[mystate][myaction] = gamma*lmbda*e[mystate][myaction]\n",
    "                    policy[mystate] = epsilon_soft_state_specific(env, mystate[0], Q[mystate], epsilon = 0.25)\n",
    "                \n",
    "                if done == True:\n",
    "                    break\n",
    "                \n",
    "                state = next_state[:]\n",
    "                action = next_action\n",
    "        \n",
    "        self.policy = policy.copy() # update the agent's policy\n",
    "        self.greedy_policy = dict((key,np.argmax(value)) for key, value in policy.items()) # In the end, take best policy with prob 1\n",
    "        self.Q = Q.copy()\n",
    "        self.e = e.copy()\n",
    "        self.cumulative_reward = cumulative_reward\n",
    "        # V_on = dict((key,np.max(value)) for key, value in Q.items()) # Take best Q approximation as well\n",
    "        # return on_policy, V_on, Q. # Don't need to return anything for now. Just update the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82a4e0",
   "metadata": {},
   "source": [
    "### Training Agent 3\n",
    "I start by training it against the `negamax` agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "561a845e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b65fd1acf71411988694a113e040a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [295]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer1 \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtrain([\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegamax\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m trainer2 \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mtrain([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegamax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mAgent3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_TabularSARSA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [293]\u001b[0m, in \u001b[0;36mTabularSARSA.learn_TabularSARSA\u001b[0;34m(self, trainer_mark1, trainer_mark2, num_episodes, gamma, epsilon, alpha, lmbda)\u001b[0m\n\u001b[1;32m     96\u001b[0m         Q[mystate][myaction] \u001b[38;5;241m=\u001b[39m Q[mystate][myaction] \u001b[38;5;241m+\u001b[39m alpha\u001b[38;5;241m*\u001b[39mdelta\u001b[38;5;241m*\u001b[39me[mystate][myaction]\n\u001b[1;32m     97\u001b[0m         e[mystate][myaction] \u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m*\u001b[39mlmbda\u001b[38;5;241m*\u001b[39me[mystate][myaction]\n\u001b[0;32m---> 98\u001b[0m     policy[mystate] \u001b[38;5;241m=\u001b[39m \u001b[43mepsilon_soft_state_specific\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmystate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmystate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Input \u001b[0;32mIn [219]\u001b[0m, in \u001b[0;36mepsilon_soft_state_specific\u001b[0;34m(env, board, Q_state, epsilon)\u001b[0m\n\u001b[1;32m     62\u001b[0m         new_policy[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     64\u001b[0m valid_Q_state \u001b[38;5;241m=\u001b[39m Q_state[:]   \u001b[38;5;66;03m# I think this is a messy way to do this but it should work. \u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_Q_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m valid_moves:\n\u001b[1;32m     67\u001b[0m         valid_Q_state[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000000\u001b[39m \u001b[38;5;66;03m# Set Q for illegal moves extremely low so it won't be max\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Agent3 = TabularSARSA()\n",
    "\n",
    "trainer1 = env.train([None, \"negamax\"])\n",
    "trainer2 = env.train([\"negamax\", None])\n",
    "\n",
    "Agent3.learn_TabularSARSA(trainer1, trainer2, num_episodes = 25000, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85db9e",
   "metadata": {},
   "source": [
    "Next, I train it against a random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5057d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9baf973109447afb6c3d51601303c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer1 = env.train([None, \"random\"])\n",
    "trainer2 = env.train([\"random\", None])\n",
    "\n",
    "Agent3.learn_TabularSARSA(trainer1, trainer2, num_episodes = 25000, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89fc69",
   "metadata": {},
   "source": [
    "I'll give it another round of training, this time against a (soft policy) version of itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c68301d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eda084d58da4938bf899345e589faca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mypolicy = Agent3.policy\n",
    "\n",
    "def soft_training_policy(obs): # now defined according to the initial agent policy\n",
    "    board_id = (tuple(obs.board), obs.mark,)\n",
    "    if board_id in mypolicy.keys():\n",
    "        action = int(np.random.choice(range(5), p = mypolicy[board_id]))\n",
    "        return action\n",
    "    else:\n",
    "        valid_moves = [col for col in range(5) if obs.board[col] == 0]\n",
    "        n_valid_moves = len(valid_moves) # number of valid moves\n",
    "        action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "        action = int(action) # Make it an integer, not a numpy integer\n",
    "        #print('playing random')\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "trainer1 = env.train([None, soft_training_policy])\n",
    "trainer2 = env.train([soft_training_policy, None])\n",
    "\n",
    "Agent3.learn_TabularSARSA(trainer1, trainer2, num_episodes = 10000, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d80a8",
   "metadata": {},
   "source": [
    "### Plot rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_reward = np.cumsum(Agent3.cumulative_reward)\n",
    "plot_rewards(cum_reward, method = \"Agent 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647adff9",
   "metadata": {},
   "source": [
    "### Testing Agent 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb31c0c",
   "metadata": {},
   "source": [
    "The training ran. Let's now see how well it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b398f712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: false,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: false,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;96048d4e-dfb0-11ed-af15-f018989b6354&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;rows&quot;: 4,\n",
       "      &quot;columns&quot;: 5,\n",
       "      &quot;inarow&quot;: 3,\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      0,\n",
       "      0\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;ACTIVE&quot;,\n",
       "      &quot;INACTIVE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    []\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;interactive&quot;: true\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"300\" height=\"300\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve its policy, turn it into a playing strategy for `env.play()`\n",
    "\n",
    "mypolicy = Agent3.greedy_policy\n",
    "\n",
    "def trained_policy(obs): # now defined according to the initial agent policy\n",
    "    board_id = (tuple(obs.board), obs.mark,)\n",
    "    if board_id in mypolicy.keys():\n",
    "        action = int(mypolicy[board_id])\n",
    "        return action\n",
    "    else:\n",
    "        valid_moves = [col for col in range(5) if obs.board[col] == 0]\n",
    "        n_valid_moves = len(valid_moves) # number of valid moves\n",
    "        action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "        action = int(action) # Make it an integer, not a numpy integer\n",
    "        #print('playing random')\n",
    "        return action\n",
    "\n",
    "env.play([None, trained_policy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1168f1d",
   "metadata": {},
   "source": [
    " Let's now see how it does against random and negamax when playing according to this policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "38cb52ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd9c7e8b7274853b51971852479cded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(77, 0.77)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, function sim_games() runs n games between player 1 and player 2 (rotating who goes first)\n",
    "def sim_games(player1, player2, n):\n",
    "    p1_wins = 0\n",
    "    for i in tqdm(range(n)):\n",
    "        if i % 2 == 0:\n",
    "            game = env.run([player1, player2])\n",
    "            final_state = game[len(game) - 1][0]\n",
    "            if final_state.reward == 1: # if the game ended and p1 was the last to play, then he won\n",
    "                p1_wins += 1\n",
    "                \n",
    "        else:\n",
    "            game = env.run([player2, player1])\n",
    "            final_state = game[len(game) - 1][1]\n",
    "            if final_state.reward == 1: # if the game ended and p1 was the last to play, then he won\n",
    "                p1_wins += 1\n",
    "    return p1_wins, p1_wins/n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b151d",
   "metadata": {},
   "source": [
    "#### vs. random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "Agent3_vs_random = sim_games(trained_policy, 'random', 1000)\n",
    "Agent3_vs_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c776851",
   "metadata": {},
   "source": [
    "#### vs. negamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01b23997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548bf369d74e4e799f41424fb4fbb082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(63, 0.63)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(234)\n",
    "Agent3_vs_negamax = sim_games(trained_policy, 'negamax', 100)\n",
    "Agent3_vs_negamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8567d",
   "metadata": {},
   "source": [
    "#### vs. me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e921319",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play([trained_policy, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41718078",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play([None, trained_policy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df518e",
   "metadata": {},
   "source": [
    "It works! It is better than the random and negamax players. The agent does pretty well. But can we make it even better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d152828",
   "metadata": {},
   "source": [
    "# Agent 4 - Linear value function approximation - Semi-gradient n-step SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa21a09",
   "metadata": {},
   "source": [
    "Agent 3 (Tabular SARSA(位)) worked reasonably well. Like Agent 2 (MC Method), it learned a policy that could be used to consistently beat the random agent and also beat the `negamax` agent over 50% of the time. Still, both of these agent's abilities are limited by the the direct dictionary lookup that is required for each state. This game has far too many states to efficiently be explored, and these tabular policy updating methods have no way of using the information it knows about seen states when it encounters new states.\n",
    "\n",
    "Thus, I turn to a method that uses value function approximation to utilize information from seen states when deciding what to do for an unseen state. Note that Agent 2 only played randomly when it encountered an unseen state. In this section, I implement an n-step SARSA method with linear value function approximation. To do this, I define a set of features that any state (seen or unseen) can be characterized by, and use a linear combination of these features to estimate the value of each state.\n",
    "\n",
    "Note, this roughly follows the algorithm shown in Lecture 7.\n",
    "\n",
    "The helper functions and environment setup are already created under Agent 3, so I go directly to creating the agent class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74f4e3",
   "metadata": {},
   "source": [
    "### Selecting features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55814ae3",
   "metadata": {},
   "source": [
    "An important task is now to decide how to define features based on the Connect four environment. There does not seem to be a straightforward answer for a game this complex. Thus, to approach this problem, I thought of what factors \\emph{I} use when deciding how to move in connect four. Some key things I consider are how many \"two in a rows\" I have, how many \"two in a rows\" my opponent has (a \"two in a row\" requires not only two pieces in a row, but also a free space to make it three). Really, I think these two alone are a good starting point. Trying to make \"two in a rows\" and prevent the other person from doing the same is a good strategy to win connect four."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e253576",
   "metadata": {},
   "source": [
    "### Helper Function: Count the number of \"two in a rows\" in a state:\n",
    "\n",
    "After initially only counting two in a rows, I now want to count three in a rows (for the player only) too. This seems redundant, as the game will be over if there is a three in a row, but counting them allows the to see if a certain action will make a three in a row before actually playing it. This helps for the $\\hat{q}(S,A, \\mathbf{w})$ function later on. \n",
    "\n",
    "Inputs:\n",
    "\n",
    "- env: The game environment\n",
    "- state: A state (in the `state_id()` format I defined earlier)\n",
    "\n",
    "Returns:\n",
    "- 0: the number of two in a rows on the board (with playable spot to make three in a row) for the player\n",
    "- 1: the number of two in a rows on the board (with playable spot to make three in a row) for the opponent\n",
    "- 2: the number of three in a rows on the board for the player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "d57cd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TwoInARows(env, state):\n",
    "    nA = env.configuration.columns \n",
    "    nR = env.configuration.rows\n",
    "    mymark = int(state[1]) # will be either 1 or 2\n",
    "    if mymark == 1:\n",
    "        oppmark = 2\n",
    "    else:\n",
    "        oppmark = 1 # Set opponent mark\n",
    "    board = list(state[0]) # list so I can edit playable spaces below\n",
    "    \n",
    "    \n",
    "    # Now make an edit to the board identifying PLAYABLE spaces - we will label these with mark 3\n",
    "    for i in range(len(board)):\n",
    "        if board[i] == 0:\n",
    "            if i in range(nA*nR-nA, nA*nR): # the first row is treated differently - no rows below\n",
    "                board[i] = 3\n",
    "            else:\n",
    "                if board[i + nA] == 1 or board[i + nA] == 2: # if a zero has a mark in the spot below, it's playable\n",
    "                    board[i] = 3\n",
    "                \n",
    "    board = tuple(board[:])\n",
    "    \n",
    "    global myboard\n",
    "    myboard = board[:]\n",
    "    ## This is probably inefficient, but the best way I can think of is to go row by row, col by col, and then across diag\n",
    "    \n",
    "    \n",
    "    ### Start with rows\n",
    "    n_two_in_a_row_player = 0\n",
    "    n_two_in_a_row_opponent = 0\n",
    "    n_three_in_a_row_player = 0\n",
    "    \n",
    "    for row in reversed(range(nR)): # reversed because row 1 is the last 5 digits of the tuple 'board'\n",
    "        current_row = board[5*row:5*(row+1)] # i.e. when row = 1, it is\n",
    "        for i in range(nA-1): # only need to check first four entries because each is checked against i + 1\n",
    "            if current_row[i] == mymark:\n",
    "                if current_row[i] == current_row[i+1]: # if two in a row are mark\n",
    "                    if i == 0:\n",
    "                        if current_row[i+2] == 3: # and the next is playable\n",
    "                            n_two_in_a_row_player += 1\n",
    "                        if current_row[i+2] == mymark: # and the next is also mark, ie three in a row\n",
    "                            n_three_in_a_row_player = 1\n",
    "                    elif i == (nA-2):\n",
    "                        if current_row[i-1] == 3: # and the previous is playable\n",
    "                            n_two_in_a_row_player += 1\n",
    "                        if current_row[i-1] == mymark:\n",
    "                            n_three_in_a_row_player = 1\n",
    "                    else:\n",
    "                        if current_row[i-1] == 3: # and there is a playable spot to the left\n",
    "                            n_two_in_a_row_player += 1\n",
    "                        elif current_row[i-1] == 3: # and there is a playable spot to the left\n",
    "                            n_three_in_a_row_player = 1\n",
    "                        if current_row[i+2] == 3: # and there is a playable spot to the right (left and right counts as 2)\n",
    "                            n_two_in_a_row_player += 1\n",
    "                        elif current_row[i+2] == 3:\n",
    "                            n_three_in_a_row_player = 1\n",
    "                \n",
    "                if i < nA - 2: # for all but last two rows, I need to check for mark blank mark, which is essentially\n",
    "                                # the same as a two in a row\n",
    "                        if current_row[i] == current_row[i+2] and current_row[i+1] == 3:\n",
    "                            n_two_in_a_row_player += 1\n",
    "                            \n",
    "                            \n",
    "                           \n",
    "            elif current_row[i] == oppmark:\n",
    "                if current_row[i] == current_row[i+1]: # if two in a row are mark\n",
    "                    if i == 0:\n",
    "                        if current_row[i+2] == 3: # and the next is empty\n",
    "                            n_two_in_a_row_opponent += 1\n",
    "                    elif i == (nA-2):\n",
    "                        if current_row[i-1] == 3: # and the previous is empty\n",
    "                            n_two_in_a_row_opponent += 1\n",
    "                    else:\n",
    "                        if current_row[i-1] == 3: # and there is a playable spot to the left\n",
    "                            n_two_in_a_row_opponent += 1\n",
    "                        if current_row[i+2] == 3: # and there is a playable spot to the right\n",
    "                            n_two_in_a_row_opponent += 1\n",
    "                            \n",
    "                if i < nA - 2: # for all but last two rows, I need to check for mark blank mark, which is essentially\n",
    "                                # the same as a two in a row\n",
    "                        if current_row[i] == current_row[i+2] and current_row[i+1] == 3:\n",
    "                            n_two_in_a_row_opponent += 1\n",
    "                            \n",
    "    ### Next columns\n",
    "    for col in range(nA):\n",
    "        current_col = board[col::5] # every five spots to get a column\n",
    "        for i in range(2, nR): # only need to check the last two spots. Remember first spot is top row.\n",
    "                                # and there must be open spot above spot i - 1. \n",
    "            if current_col[i] == mymark:\n",
    "                if current_col[i] == current_col[i-1]:\n",
    "                    if current_col[i-2] == 3:\n",
    "                        n_two_in_a_row_player += 1\n",
    "                    elif current_col[i-2] == mymark:\n",
    "                        n_three_in_a_row_player = 1\n",
    "                        \n",
    "            if current_col[i] == oppmark:\n",
    "                if current_col[i] == current_col[i-1] and current_col[i-2] == 3:\n",
    "                    n_two_in_a_row_opponent += 1\n",
    "                    \n",
    "    ### Next, diagonals.\n",
    "        # Note that there are only four diagonals on a 4x5 board that are of interest (three spots or longer)\n",
    "        # each way, so 8 total\n",
    "        \n",
    "    # Rather than spend time finding a formula for the indexes, I'm just going to pull the diags myself\n",
    "    # Note order should be same for all - in this case I am top to bottom so if i and i + 1 are equal, then\n",
    "    # I am looking to see if i - 1 is an open slot.\n",
    "    diag = [0,0,0,0,0,0,0,0]\n",
    "    diag[0] = [board[i] for i in [2,6,10]]\n",
    "    diag[1] = [board[i] for i in [3,7,11,15]]\n",
    "    diag[2] = [board[i] for i in [4,8,12,16]]\n",
    "    diag[3] = [board[i] for i in [9,13,17]]\n",
    "    diag[4] = [board[i] for i in [2,8,14]]\n",
    "    diag[5] = [board[i] for i in [1,7,13,19]]\n",
    "    diag[6] = [board[i] for i in [0,6,12,18]]\n",
    "    diag[7] = [board[i] for i in [5,11,17]]\n",
    "    \n",
    "    global diagtest\n",
    "    diagtest = diag[3]\n",
    "    \n",
    "    for mydiag in range(len(diag)):\n",
    "        current_diag = diag[mydiag]\n",
    "        for i in range(2,len(current_diag)):\n",
    "            if current_diag[i] == mymark:\n",
    "                if current_diag[i] == current_diag[i-1]:\n",
    "                    if current_diag[i-2] == 3:\n",
    "                        n_two_in_a_row_player += 1\n",
    "                    elif current_diag[i-2] == mymark:\n",
    "                        n_three_in_a_row_player = 1\n",
    "                        \n",
    "            if current_diag[i] == oppmark:\n",
    "                if current_diag[i] == current_diag[i-1] and current_diag[i-2] == 3:\n",
    "                    n_two_in_a_row_opponent += 1\n",
    "                    \n",
    "    return n_two_in_a_row_player, n_two_in_a_row_opponent, n_three_in_a_row_player      \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "cefb52ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ((0,0,0,0,0,1,0,0,0,0,2,0,0,0,0,1,2,0,2,0),1)\n",
    "TwoInARows(env, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb67479",
   "metadata": {},
   "source": [
    "Now we can use this function to take any state, seen or unseen, and map it to a parameter vector $\\vec{\\theta}$ =  which holds 1) the number of two in a rows for the player and 2) the number of two in a rows for the opponent. We can then use this parameter vector and associated weights (which will be updated using reinforcement learning) to approximate the value of any state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972bc8d",
   "metadata": {},
   "source": [
    "### Helper function: $\\hat{q}$\n",
    "\n",
    "We need some new helper functions to deal with the parameters and weights for each class.\n",
    "\n",
    "There are some new attributes that we need to define for this agent who learns using semi-gradient SARSA. Most importantly, we have the weights, which I initialize as (0,0), and a differentiable function parameterization $\\hat{q}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$. \n",
    "\n",
    "I will use a simple form for $\\hat{q}$, which takes the the number of two in a rows for the player and the opponent on board created by the state, action combination and takes their linear combination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "8ba3d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_hat(env, state, action, w):\n",
    "    nA = env.configuration.columns\n",
    "    nR = env.configuration.rows\n",
    "    \n",
    "    # Now edit board to what it will look like if action is taken\n",
    "    board = list(state[0])\n",
    "    mymark = state[1]\n",
    "    \n",
    "        # find lowest 0 in that column of the action, change it to my mark\n",
    "    playing_col = board[action::nA]\n",
    "    playing_row = 0 # default to bottom row\n",
    "    for i in range(len(playing_col)):\n",
    "        if playing_col[i] > 0: # find first non-zero (if there is one). PLAYING_COL[0] SHOULD NEVER BE > 0.\n",
    "            playing_row = nR - i # playing row \n",
    "            break \n",
    "    \n",
    "    board[action + 5*(nR - 1 - playing_row)] = mymark\n",
    "    \n",
    "    board = tuple(board[:])\n",
    "    new_state = (board, mymark)\n",
    "    \n",
    "    \n",
    "    # Now find number of two in a rows in new state\n",
    "    state_features = TwoInARows(env, new_state)\n",
    "    \n",
    "    # And generate q_hat\n",
    "    q_hat = np.dot(list(state_features), w) # linear combination of features with weights w.\n",
    "    \n",
    "    return q_hat, state_features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14720be",
   "metadata": {},
   "source": [
    "Now we also want a function to compute the gradient vector $\\Delta \\hat{q}(S, A, \\mathbf{w})$ = $\\left( \\frac{\\partial \\hat{q}(S, A, \\mathbf{w})}{\\partial w_1}, \\frac{\\partial \\hat{q}(S, A, \\mathbf{w})}{\\partial w_2}, \\frac{\\partial \\hat{q}(S, A, \\mathbf{w})}{\\partial w_3}\\right)^T$. This is actually fairly straightforward for the simple linear combination that I am using here, as the derrivative with respect to the weights $\\mathbf{w}$ is just the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4b94da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_gradient(state_features):\n",
    "    return np.array(state_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264c724",
   "metadata": {},
   "source": [
    "### Create the Agent 4/5 Class\n",
    "\n",
    "Note: after I created Agent 4, I decided to go ahead and add SARSA($\\lambda$) so I just added it to the same agent class. When I actually train an agent, I will use different instances of this class for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "544beff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiGradientSARSA(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        from collections import defaultdict\n",
    "        self.epsilon = 0.25\n",
    "        self.w = np.zeros(3)\n",
    "        self.env = make(\"connectx\", {\"rows\": 4, \"columns\": 5, \"inarow\": 3}, steps=[], debug=debug_mode)\n",
    "        self.cumulative_reward = []\n",
    "\n",
    "    def act(self, obs):  # play (greedily) according to the current policy. Note that it only plays one turn. \n",
    "        board_id = (tuple(obs.board), obs.mark,)\n",
    "        # Find estimate for q for each valid action\n",
    "        board = board_id[0]\n",
    "        valid_moves = [col for col in range(5) if board[col] == 0]\n",
    "        \n",
    "        best_move = np.random.choice(valid_moves) # initialize randomly\n",
    "        \n",
    "        q_estimate = [0,0,0,0,0] # initialize\n",
    "        for i in range(5):\n",
    "            if i in valid_moves:\n",
    "                q_estimate[i] = q_hat(self.env, board_id, an_action, self.w)[0]\n",
    "            else: \n",
    "                q_estimate[i] = -100000 # make it a tiny number otherwise, so invalid moves won't be picked\n",
    "        \n",
    "        action = int(np.argmax(q_estimate))\n",
    "        return action\n",
    "        \n",
    "        \n",
    "    def choose_action_greedily(self, state): # different from act in that this sees a state in my format (via state_id)\n",
    "        # Find estimate for q for each valid action\n",
    "        board = state[0]\n",
    "        valid_moves = [col for col in range(5) if board[col] == 0]\n",
    "        \n",
    "        best_move = np.random.choice(valid_moves) # initialize randomly\n",
    "        \n",
    "        q_estimate = [0,0,0,0,0] # initialize\n",
    "        for i in range(5):\n",
    "            if i in valid_moves:\n",
    "                q_estimate[i] = q_hat(self.env, state, an_action, self.w)[0]\n",
    "            else: \n",
    "                q_estimate[i] = -100000 # make it a tiny number otherwise, so invalid moves won't be picked\n",
    "        \n",
    "        action = int(np.argmax(q_estimate))\n",
    "        return action\n",
    "    \n",
    "    def choose_action_nongreedily(self, state, w): # different from act in that this sees a state in my format (via state_id)\n",
    "        # Find estimate for q for each valid action\n",
    "        board = state[0]\n",
    "        valid_moves = [col for col in range(5) if board[col] == 0]\n",
    "        n_valid_moves = len(valid_moves)\n",
    "        epsilon = self.epsilon\n",
    "        \n",
    "        best_move = np.random.choice(valid_moves) # initialize randomly\n",
    "        \n",
    "        q_estimate = [0,0,0,0,0] # initialize\n",
    "        for i in range(5):\n",
    "            if i in valid_moves:\n",
    "                q_estimate[i] = q_hat(self.env, state, i, w)[0]\n",
    "            else: \n",
    "                q_estimate[i] = -100000 # make it a tiny number otherwise, so invalid moves won't be picked\n",
    "        \n",
    "\n",
    "        best_action = int(np.argmax(q_estimate))\n",
    "        \n",
    "        # now define prob vectors\n",
    "        probs = np.ones(5)*(epsilon/n_valid_moves)\n",
    "        for i in range(5):\n",
    "            if i not in valid_moves:\n",
    "                probs[i] = 0 # set invalid move probability to 0\n",
    "    \n",
    "        probs[best_action] = 1 - epsilon + epsilon/n_valid_moves\n",
    "        \n",
    "        # And now choose action\n",
    "        action = int(np.random.choice(range(5), p = probs))\n",
    "        \n",
    "        return action\n",
    "        \n",
    "            \n",
    "\n",
    "    def learn_SemiGradientnSARSA(self, trainer_mark1, trainer_mark2, num_episodes = 10000, \n",
    "                         gamma = 0.9, alpha = 0.1, n = 3): \n",
    "                \n",
    "        # Initialize env and w values\n",
    "        env = self.env\n",
    "        w = self.w.copy()\n",
    "        cumulative_reward = self.cumulative_reward.copy()\n",
    "        \n",
    "        nA = env.configuration.columns # number of actions is the number of columns on the board\n",
    "        nR = env.configuration.rows\n",
    "    \n",
    "        # loop over episodes\n",
    "        for i in tqdm(range(num_episodes)): # tgdm gives progress bar\n",
    "            episode = [] # initialize the episode as an empty list. We will add to it later\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                state = state_id(trainer_mark1.reset()) # empty board\n",
    "                current_trainer = trainer_mark1\n",
    "                mark = int(1)\n",
    "            else:\n",
    "                state = state_id(trainer_mark2.reset()) # empty board\n",
    "                current_trainer = trainer_mark2\n",
    "                mark = int(2)\n",
    "                \n",
    "  \n",
    "            # Initialize action\n",
    "            action = self.choose_action_nongreedily(state, w)\n",
    "                                    # Don't have to worry about illegal first action\n",
    "            \n",
    "            \n",
    "            # Initialize T as maximum number of moves + 1\n",
    "            T = (nA*nR)/2 + 1\n",
    "            \n",
    "            # Initialize t = 0\n",
    "            t = 0\n",
    "            \n",
    "            \n",
    "            # Repeat, for each step in the episode:\n",
    "            while True: \n",
    "                \n",
    "                if t < T:\n",
    "                    next_state_raw, reward, done, info = current_trainer.step(action)\n",
    "                    next_state = state_id(next_state_raw)\n",
    "            \n",
    "                    if done == True: # If step is terminal\n",
    "                        T = t + 1  \n",
    "                        next_action = action\n",
    "                        cumulative_reward.append(reward)\n",
    "                    else:\n",
    "                        next_action = self.choose_action_nongreedily(next_state, w)\n",
    "                    \n",
    "                    episode.append((state, action, reward, next_state, next_action))\n",
    "                \n",
    "                tau = t - n + 1 # tau is the time whose estimate is being updated\n",
    "                     \n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    for i in range(tau + 1, min(tau + n, T) + 1):   # min between the number of future steps and when episode finishes\n",
    "                        G += (gamma**(i - tau - 1)) * episode[i-1][2] # this is the reward\n",
    "                    if tau + n < T:\n",
    "                        G += (gamma**(n)) * q_hat(env, episode[tau + n - 1][3], episode[tau + n - 1][4], w)[0]\n",
    "                        # So tau + n - 1 is the last turn in 'episode', but I want the next state, action after\n",
    "                    \n",
    "                    # Now update w\n",
    "                    my_q_hat = q_hat(env, episode[tau][0], episode[tau][1], w)\n",
    "                    w += alpha * (G - my_q_hat[0])*q_gradient(my_q_hat[1]) \n",
    "                \n",
    "                t += 1 # move t up one\n",
    "                if tau == T - 1:\n",
    "                    break\n",
    "                action = next_action\n",
    "                state = next_state\n",
    "\n",
    "        \n",
    "        self.w = w.copy() # update the agent's w\n",
    "        self.cumulative_reward = cumulative_reward.copy()\n",
    "    \n",
    "    def learn_SemiGradientSARSAlambda(self, trainer_mark1, trainer_mark2, num_episodes = 10000, \n",
    "                         gamma = 0.9, alpha = 0.1, lmbda = 0.5): \n",
    "                \n",
    "        # Initialize env and w values\n",
    "        env = self.env\n",
    "        w = self.w.copy()\n",
    "        e = np.zeros(3) # e is also a vector\n",
    "        cumulative_reward = self.cumulative_reward.copy()\n",
    "        \n",
    "        nA = env.configuration.columns # number of actions is the number of columns on the board\n",
    "        nR = env.configuration.rows\n",
    "        \n",
    "        global mye\n",
    "        mye = []\n",
    "        \n",
    "        global myq\n",
    "        myq = []\n",
    "        \n",
    "        global myep\n",
    "        myep = []\n",
    "\n",
    "    \n",
    "        # loop over episodes\n",
    "        for i in tqdm(range(num_episodes)): # tgdm gives progress bar\n",
    "            episode = [] # initialize the episode as an empty list. We will add to it later\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                state = state_id(trainer_mark1.reset()) # empty board\n",
    "                current_trainer = trainer_mark1\n",
    "                mark = int(1)\n",
    "            else:\n",
    "                state = state_id(trainer_mark2.reset()) # empty board\n",
    "                current_trainer = trainer_mark2\n",
    "                mark = int(2)\n",
    "                \n",
    "  \n",
    "            # Initialize action\n",
    "            action = self.choose_action_nongreedily(state, w)\n",
    "                                    # Don't have to worry about illegal first action\n",
    "            \n",
    "            # Repeat, for each step in the episode:\n",
    "            while True: \n",
    "                next_state_raw, reward, done, info = current_trainer.step(action)\n",
    "                next_state = state_id(next_state_raw)\n",
    "                \n",
    "                # update e\n",
    "                my_q = q_hat(env, state, action, w)\n",
    "                e = gamma * lmbda * e + q_gradient(my_q[1]) # here I am using an accumulating traces approach\n",
    "                \n",
    "                myq.append(my_q[0])\n",
    "                \n",
    "                mye.append(e)\n",
    "                \n",
    "                myep.append(my_q[0])\n",
    "                \n",
    "            \n",
    "                if done == True: # If step is terminal\n",
    "                    #next_state = state[:] # if this is a terminal state, there is no next_state\n",
    "                    #next_action = action.copy()\n",
    "                    w += alpha * (reward  - my_q[0])*e\n",
    "                    cumulative_reward.append(reward)\n",
    "                    \n",
    "                else:\n",
    "                    next_action = self.choose_action_nongreedily(next_state, w)\n",
    "                    w += alpha * (reward + gamma * q_hat(env, next_state, next_action, w)[0] -\n",
    "                                 my_q[0])*e\n",
    "\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                state = next_state[:]\n",
    "                action = next_action\n",
    "\n",
    "        \n",
    "        self.w = w.copy() # update the agent's w\n",
    "        self.cumulative_reward = cumulative_reward.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703661f",
   "metadata": {},
   "source": [
    "### Training Agent 4: Semi-gradient n-step SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "91d7abd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba377eab9a714244948ba1052078cc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Agent4 = SemiGradientSARSA()\n",
    "\n",
    "trainer1 = env.train([None, \"negamax\"])\n",
    "trainer2 = env.train([\"negamax\", None])\n",
    "\n",
    "Agent4.learn_SemiGradientnSARSA(trainer1, trainer2, num_episodes = 25000, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "485a5f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d31fac99fa430f9d307e6a9d617c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer1 = env.train([None, \"random\"])\n",
    "trainer2 = env.train([\"random\", None])\n",
    "\n",
    "Agent4.learn_SemiGradientnSARSA(trainer1, trainer2, num_episodes = 25000, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b9356",
   "metadata": {},
   "source": [
    "I'll give it another round of training against the soft-policy version of Agent 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f498eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypolicy = Agent3.policy\n",
    "\n",
    "def soft_training_policy(obs): # now defined according to the initial agent policy\n",
    "    board_id = (tuple(obs.board), obs.mark,)\n",
    "    if board_id in mypolicy.keys():\n",
    "        action = int(np.random.choice(range(5), p = mypolicy[board_id]))\n",
    "        return action\n",
    "    else:\n",
    "        valid_moves = [col for col in range(5) if obs.board[col] == 0]\n",
    "        n_valid_moves = len(valid_moves) # number of valid moves\n",
    "        action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "        action = int(action) # Make it an integer, not a numpy integer\n",
    "        #print('playing random')\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "trainer1 = env.train([None, soft_training_policy])\n",
    "trainer2 = env.train([soft_training_policy, None])\n",
    "\n",
    "Agent4.learn_SemiGradientnSARSA(trainer1, trainer2, num_episodes = 10000, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae08c9",
   "metadata": {},
   "source": [
    "Now plot cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "24691971",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SemiGradientSARSA' object has no attribute 'cumulative_reward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [599]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_rewards(\u001b[43mAgent4\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumulative_reward\u001b[49m, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent 4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SemiGradientSARSA' object has no attribute 'cumulative_reward'"
     ]
    }
   ],
   "source": [
    "cum_reward = np.cumsum(Agent4.cumulative_reward)\n",
    "plot_rewards(cum_reward, method = \"Agent 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d62cf",
   "metadata": {},
   "source": [
    "### Testing Agent 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "90a695a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve its policy, turn it into a playing strategy for `env.play()`\n",
    "\n",
    "myw = Agent4.w\n",
    "\n",
    "def trained_policy(obs): # now defined according to the initial agent policy\n",
    "    board_id = (tuple(obs.board), obs.mark,)\n",
    "    # Find estimate for q for each valid action\n",
    "    board = board_id[0]\n",
    "    valid_moves = [col for col in range(5) if board[col] == 0]\n",
    "        \n",
    "    best_move = np.random.choice(valid_moves) # initialize randomly\n",
    "        \n",
    "    q_estimate = [0,0,0,0,0] # initialize\n",
    "    for i in range(5):\n",
    "        if i in valid_moves:\n",
    "            q_estimate[i] = q_hat(env, board_id, i, myw)[0]\n",
    "        else: \n",
    "            q_estimate[i] = -100000 # make it a tiny number otherwise, so invalid moves won't be picked\n",
    "        \n",
    "    global test\n",
    "    test = q_estimate\n",
    "    action = int(np.argmax(q_estimate))\n",
    "    return action\n",
    "\n",
    "#env.play([None, trained_policy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dbc4e",
   "metadata": {},
   "source": [
    "#### vs. Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "17dd67db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191c11bcf17e4cb1a7cf6bd7dbd4e417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(94, 0.94)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(345)\n",
    "Agent4_vs_random = sim_games(trained_policy, 'random', 100)\n",
    "Agent4_vs_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de6ba9",
   "metadata": {},
   "source": [
    "#### vs. Negamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "f22522e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b2e3219d0f41559e037e21bcaad120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(13, 0.13)"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(345)\n",
    "Agent4_vs_negamax = sim_games(trained_policy, 'negamax', 100)\n",
    "Agent4_vs_negamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdaceba",
   "metadata": {},
   "source": [
    "#### vs. me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa205ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play([None, trained_policy])\n",
    "env.play([trained_policy, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97518cc8",
   "metadata": {},
   "source": [
    "### Training Agent 5\n",
    "\n",
    "I also added an extension of Gradient-descent SARSA() to the code for the SemiGradientSARSA agent, roughly following the algorithm is PGT Lecture III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "77404a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a09fa2e1e1f4f8199bfbdcdb8f9d4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here I train on negamax\n",
    "Agent5 = SemiGradientSARSA()\n",
    "\n",
    "trainer1 = env.train([None, \"negamax\"])\n",
    "trainer2 = env.train([\"negamax\", None])\n",
    "\n",
    "Agent5.learn_SemiGradientSARSAlambda(trainer1, trainer2, num_episodes = 25000, gamma = 0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3614b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I train on random\n",
    "Agent5 = SemiGradientSARSA()\n",
    "\n",
    "trainer1 = env.train([None, \"random\"])\n",
    "trainer2 = env.train([\"random\", None])\n",
    "\n",
    "Agent5.learn_SemiGradientSARSAlambda(trainer1, trainer2, num_episodes = 25000, gamma = 0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda68ec",
   "metadata": {},
   "source": [
    "Now I train on a soft-policy version of Agent 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a63bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypolicy = Agent3.policy\n",
    "\n",
    "def soft_training_policy(obs): # now defined according to the initial agent policy\n",
    "    board_id = (tuple(obs.board), obs.mark,)\n",
    "    if board_id in mypolicy.keys():\n",
    "        action = int(np.random.choice(range(5), p = mypolicy[board_id]))\n",
    "        return action\n",
    "    else:\n",
    "        valid_moves = [col for col in range(5) if obs.board[col] == 0]\n",
    "        n_valid_moves = len(valid_moves) # number of valid moves\n",
    "        action = np.random.choice(valid_moves, p = np.ones(n_valid_moves)/n_valid_moves) \n",
    "        action = int(action) # Make it an integer, not a numpy integer\n",
    "        #print('playing random')\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "trainer1 = env.train([None, soft_training_policy])\n",
    "trainer2 = env.train([soft_training_policy, None])\n",
    "\n",
    "Agent5.learn_SemiGradientSARSAlambda(trainer1, trainer2, num_episodes = 10000, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360dbf3",
   "metadata": {},
   "source": [
    "Now plot cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "54b7231a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_reward = np.cumsum(Agent5.cumulative_reward)\n",
    "plot_rewards(cum_reward, method = \"Agent 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b6d5a",
   "metadata": {},
   "source": [
    "### Testing Agent 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "b2c158de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve its policy, turn it into a playing strategy for `env.play()`\n",
    "\n",
    "myw = Agent5.w\n",
    "\n",
    "def trained_policy(obs): # now defined according to the initial agent policy\n",
    "    board_id = (tuple(obs.board), obs.mark,)\n",
    "    # Find estimate for q for each valid action\n",
    "    board = board_id[0]\n",
    "    valid_moves = [col for col in range(5) if board[col] == 0]\n",
    "        \n",
    "    best_move = np.random.choice(valid_moves) # initialize randomly\n",
    "        \n",
    "    q_estimate = [0,0,0,0,0] # initialize\n",
    "    for i in range(5):\n",
    "        if i in valid_moves:\n",
    "            q_estimate[i] = q_hat(env, board_id, i, myw)[0]\n",
    "        else: \n",
    "            q_estimate[i] = -100000 # make it a tiny number otherwise, so invalid moves won't be picked\n",
    "        \n",
    "    global test\n",
    "    test = q_estimate\n",
    "    action = int(np.argmax(q_estimate))\n",
    "    return action\n",
    "\n",
    "#env.play([None, trained_policy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "5f676ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25562654, -0.82583998,  0.43499817])"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Agent5.w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1412e31",
   "metadata": {},
   "source": [
    "#### vs. Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "365fb3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5f91879b4d424580fb9662ad47992d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(70, 0.7)"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(345)\n",
    "Agent5_vs_random = sim_games(trained_policy, 'random', 1000)\n",
    "Agent5_vs_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2ded3",
   "metadata": {},
   "source": [
    "#### vs. Negamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "525fc1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbddcb1a41d54f0f94f580f1c280fd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(7, 0.07)"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(345)\n",
    "Agent4_vs_negamax = sim_games(trained_policy, 'negamax', 1000)\n",
    "Agent4_vs_negamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db43b03",
   "metadata": {},
   "source": [
    "#### vs. me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212eb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play([None, trained_policy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6f528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play([trained_policy, None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
